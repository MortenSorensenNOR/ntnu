\section{Background}
In order to understand how and why Pythia works, it is important to understand the basics
of reinforcement learning, and specifically, the algorithms used by the authors to achieve the 
performance of the prefetcher.

\subsection{Reinforcement Learning}
Reinforcement learning (RL) \cite{reinforcement_learning} is an area of machine learning 
that is concerned with teaching an agent an optimal set of \textit{actions} for a given 
\textit{state} in a dynamic environment through numerical \textit{rewards}. Typically 
an RL system is comprised of two parts: the agent and it's environment. The agent, 
in discrete steps $t$, observes the current \textbf{\textit{state}} of the environment 
$S_{t}$ and decides on some \textbf{\textit{action}} $A_t$. When said action is carried 
out, thereby altering the environment in some way, the environment transitions to a new 
state $S_{t+1}$, and issues a \textbf{\textit{reward}} $R_{t+1}$ based on the desirability 
of the action, which, either immediately or at some later time $t + n$, is given to the agent. 
This reward should be derived in such a way that it pushes the agent to take optimal actions
to reach the agent's true objective.

The \textbf{\textit{policy}} of an RL system defines the behavior of the agent by mapping the 
states to the actions. The goal of reinforcement learning is to learn the optimal policy that 
maximizes the cumulative reward received over time, thereby creating a mechanism through which
optimal control of the system is achieved. The expected cumulative reward for taking some action
$A$ in a state $S$ is called the \textbf{\textit{Q-value}} of the state-action pair, and is
usually denoted $Q(S, A)$. At each time step $t$, the agent iteratively optimizes it's policy
by first updating the Q-value of the state-action pair for which the current timesteps reward
pertains, and consequently optimize the current policy using the updated Q-value.

\textbf{Q-value Update.} The update process of Q-values is as follows. The agent at timestep 
$t$ observes a state $S_t$ and decideds upon an action $A_t$. This action results in a new 
state $S_{t+1}$ and a reward $R_{t+1}$, and the agent takes some new action $A_{t+1}$ in 
this new state. The Q-value of the old state-action pair $Q(S_t, A_t)$ is then updated using 
the SARSA \cite{reinforcement_learning} algorithm as shown in equation \ref{eq:sarsa}
\begin{equation}
    \begin{aligned}
        Q(S_t, A_t) &\leftarrow Q(S_t, A_t) \\
        \quad &+ \alpha\left[R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right]
    \end{aligned}
    \label{eq:sarsa}
\end{equation}
where $\alpha$ is the \textit{learning rate} parameter which controls the convergence rate of 
the Q-values on some stable solution, and $\gamma$ is the \textit{discount factor}, which 
regulates to what extent future rewards effect the decision making. A higher $\gamma$ closer 
to 1 will give a policy that is more "far-sighted", i.e. giving less significance to rewards 
in the short term and therefore the ability to value higher rewards in the future.

\textbf{Optimization Policy.} Different optimization policies will take different steps to 
find the policy for which the maximum cumulative reward is gained. A purely greedy approach 
for instance will at each step choose the action $A$ in the given state $S$ for which the 
highest Q-value $Q(S, A)$ is achieved. This may, however, leave large parts of the state 
space under-explored. It may therefore be advantageous to include a mechanism through which 
state-space exploration is encouraged. An $\epsilon$-greedy agent will \textit{stochastically} 
take some random action with some probability $\epsilon$, the \textit{exploration rate}, and 
otherwise choose the action for which the Q-value is maximized. This strikes a balance between 
exploration and optimization.

\subsection{Why Use Reinforcement Learning For Prefetching?}
Reinforcement learning, as argued by Pythia\cite{pythia}, is ideal for solving the task of 
prefetching for a few key reasons:

\textbf{Adaptive learning.}  As mentioned in section \ref{sec:introduction}, the performance 
of a prefetchers is not only dependent upon the acuracy and coverage, but also on the adverse 
effects it may have on the entire memory system, such as memory bandwidth usage. Mordern 
high-performance processors also need to be able to perform well across as wide range of 
processes, with different memory access patterns and memory needs. It is therefore clear 
that the state space that a prefetcher needs to perform well in is complex, and in order 
to achieve good performance, a prefetcher has to be able to adapt. Online reinforcement 
learning is great for exactly this, being able to dynamically adapt to changes in program 
and system behavior.

\textbf{Implementation simplicity.} Compared with other machine-learning based hardware 
prefetchers, such as NN and GNNs, which require very large cache structures and complex 
hardware for prefetch inference, Q-learning based prefetchers are simple to implement in a 
table based scheme, requireing smaller chip area and power to achieve the same prefetch 
accuracy results.
